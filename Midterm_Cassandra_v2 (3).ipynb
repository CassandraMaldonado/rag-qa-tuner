{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hVik77EiyQW",
        "outputId": "6b9f9993-193d-4e01-d921-8a0f50d55bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.96.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.47.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.47.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, pydeck, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, streamlit\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydeck-0.9.1 streamlit-1.47.0 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit openai sentence-transformers faiss-cpu requests beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List, Dict, Set\n",
        "import openai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import time\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import json\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets"
      ],
      "metadata": {
        "id": "YAyK-fUDi8EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedRAGSystem:\n",
        "    \"\"\"\n",
        "    Improved RAG system with better content extraction and answer generation.\n",
        "    Fixed to work properly with OpenAI API and give complete, accurate answers.\n",
        "    OPTIMIZED for performance - no more hanging on chunking or embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scraped_data = []\n",
        "        self.chunks = []\n",
        "        self.vector_store = None\n",
        "        self.embedding_model = None\n",
        "        self.openai_client = None\n",
        "        self.is_initialized = False\n",
        "\n",
        "        # Improved configuration\n",
        "        self.BASE_URL = \"https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/\"\n",
        "        self.CHUNK_SIZE = 800  # Larger chunks for more context\n",
        "        self.CHUNK_OVERLAP = 100  # More overlap to preserve context\n",
        "        self.EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "    def setup_openai(self, api_key: str):\n",
        "        \"\"\"Setup OpenAI client with API key.\"\"\"\n",
        "        self.openai_client = openai.OpenAI(api_key=api_key)\n",
        "        print(\"OpenAI client initialized\")\n",
        "\n",
        "    def scrape_website(self, max_pages: int = 15):\n",
        "        \"\"\"\n",
        "        Enhanced web scraping to get complete, relevant content.\n",
        "        Focus on getting full content, especially for tuition and financial info.\n",
        "        \"\"\"\n",
        "        print(f\"🕷️ Starting enhanced scraping: {self.BASE_URL}\")\n",
        "\n",
        "        session = requests.Session()\n",
        "        session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "\n",
        "        visited_urls = set()\n",
        "        urls_to_scrape = [self.BASE_URL]\n",
        "        scraped_count = 0\n",
        "\n",
        "        # Enhanced keywords - especially for tuition and financial info\n",
        "        relevant_keywords = [\n",
        "            'admission', 'admissions', 'apply', 'application',\n",
        "            'curriculum', 'courses', 'course', 'program',\n",
        "            'faculty', 'professors', 'staff',\n",
        "            'tuition', 'cost', 'financial', 'aid', 'scholarship', 'funding',\n",
        "            'requirements', 'prerequisite', 'career', 'outcomes',\n",
        "            'employment', 'student', 'life', 'experience',\n",
        "            'capstone', 'project', 'research', 'faq', 'faqs',\n",
        "            'price', 'fee', 'fees', 'payment', 'billing'\n",
        "        ]\n",
        "\n",
        "        while urls_to_scrape and scraped_count < max_pages:\n",
        "            current_url = urls_to_scrape.pop(0)\n",
        "\n",
        "            if current_url in visited_urls:\n",
        "                continue\n",
        "\n",
        "            print(f\"📄 Scraping: {current_url}\")\n",
        "\n",
        "            try:\n",
        "                response = session.get(current_url, timeout=15)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                # Enhanced content extraction\n",
        "                content_data = self._extract_enhanced_content(soup, current_url)\n",
        "                if content_data and content_data['content'] and len(content_data['content']) > 100:\n",
        "                    self.scraped_data.append(content_data)\n",
        "                    scraped_count += 1\n",
        "                    print(f\"Extracted {content_data['length']} characters from {content_data['title']}\")\n",
        "\n",
        "                visited_urls.add(current_url)\n",
        "\n",
        "                # Find relevant links (prioritize from main page)\n",
        "                if current_url == self.BASE_URL:\n",
        "                    new_links = self._find_enhanced_links(soup, self.BASE_URL, relevant_keywords)\n",
        "                    for link in new_links:\n",
        "                        if link not in visited_urls and link not in urls_to_scrape:\n",
        "                            urls_to_scrape.append(link)\n",
        "                            print(f\"Found relevant link: {link}\")\n",
        "\n",
        "                time.sleep(1)  # Be respectful\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping {current_url}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Enhanced scraping complete! Collected {len(self.scraped_data)} pages\")\n",
        "        return self.scraped_data\n",
        "\n",
        "    def _extract_enhanced_content(self, soup, url):\n",
        "        \"\"\"\n",
        "        Enhanced content extraction that specifically preserves URLs and key structured data.\n",
        "        \"\"\"\n",
        "        if not soup:\n",
        "            return None\n",
        "\n",
        "        # Remove unwanted elements\n",
        "        for element in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]):\n",
        "            element.decompose()\n",
        "\n",
        "        # Get title\n",
        "        title = \"\"\n",
        "        title_tag = soup.find('title')\n",
        "        if title_tag:\n",
        "            title = title_tag.get_text().strip()\n",
        "\n",
        "        # Extract content with special attention to URLs and structured data\n",
        "        content_parts = []\n",
        "\n",
        "        # Strategy 1: Look for main content containers\n",
        "        main_selectors = [\n",
        "            'main', '.main-content', '#main-content', '.content',\n",
        "            '.post-content', '.entry-content', '.page-content',\n",
        "            '.container', '.wrapper', '.main', 'article'\n",
        "        ]\n",
        "\n",
        "        main_content = None\n",
        "        for selector in main_selectors:\n",
        "            main_content = soup.select_one(selector)\n",
        "            if main_content:\n",
        "                break\n",
        "\n",
        "        if not main_content:\n",
        "            main_content = soup.find('body')\n",
        "\n",
        "        if main_content:\n",
        "            # Extract text while preserving URLs and important formatting\n",
        "            for element in main_content.find_all([\n",
        "                'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n",
        "                'li', 'div', 'span', 'td', 'th', 'dd', 'dt', 'a'\n",
        "            ]):\n",
        "                text = element.get_text().strip()\n",
        "\n",
        "                # Special handling for links\n",
        "                if element.name == 'a' and element.get('href'):\n",
        "                    href = element.get('href')\n",
        "                    if href.startswith('http') or href.startswith('www'):\n",
        "                        text = f\"{text} [URL: {href}]\"\n",
        "\n",
        "                # Keep longer text segments and those with important keywords\n",
        "                if text and (len(text) > 15 or any(keyword in text.lower() for keyword in [\n",
        "                    'tuition', 'cost', 'fee', 'scholarship', 'financial', 'admission',\n",
        "                    'requirement', 'course', 'program', 'capstone', 'faculty',\n",
        "                    'deadline', 'apply', 'contact', 'advisor', 'http', 'portal'\n",
        "                ])):\n",
        "                    content_parts.append(text)\n",
        "\n",
        "        # Combine content\n",
        "        content_text = \" \".join(content_parts)\n",
        "\n",
        "        # Clean up while preserving URLs\n",
        "        content_text = re.sub(r'\\s+', ' ', content_text)  # Normalize whitespace\n",
        "        content_text = content_text.strip()\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'title': title,\n",
        "            'content': content_text,\n",
        "            'length': len(content_text)\n",
        "        }\n",
        "\n",
        "    def _find_enhanced_links(self, soup, base_url, keywords):\n",
        "        \"\"\"Enhanced link finding with better keyword matching.\"\"\"\n",
        "        if not soup:\n",
        "            return []\n",
        "\n",
        "        relevant_links = []\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link.get('href')\n",
        "            link_text = link.get_text().lower().strip()\n",
        "\n",
        "            # Convert relative URLs to absolute\n",
        "            full_url = urljoin(base_url, href)\n",
        "\n",
        "            # Enhanced relevance checking\n",
        "            is_relevant = (\n",
        "                self._is_same_domain(full_url, base_url) and\n",
        "                (any(keyword in link_text for keyword in keywords) or\n",
        "                 any(keyword in href.lower() for keyword in keywords) or\n",
        "                 'faq' in href.lower() or 'tuition' in href.lower() or\n",
        "                 'cost' in href.lower() or 'financial' in href.lower())\n",
        "            )\n",
        "\n",
        "            if is_relevant:\n",
        "                relevant_links.append(full_url)\n",
        "\n",
        "        return list(set(relevant_links))\n",
        "\n",
        "    def _is_same_domain(self, url1, url2):\n",
        "        \"\"\"Check if URLs are from same domain.\"\"\"\n",
        "        try:\n",
        "            return urlparse(url1).netloc == urlparse(url2).netloc\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def create_enhanced_chunks(self):\n",
        "        \"\"\"\n",
        "        OPTIMIZED: Enhanced chunking strategy with fast regex processing.\n",
        "        No more hanging on micro-chunk creation!\n",
        "        \"\"\"\n",
        "        print(\"Creating enhanced text chunks with specialized micro-chunks...\")\n",
        "\n",
        "        self.chunks = []\n",
        "        total_docs = len(self.scraped_data)\n",
        "\n",
        "        for doc_idx, data in enumerate(self.scraped_data):\n",
        "            print(f\"Processing document {doc_idx + 1}/{total_docs}: {data['title'][:50]}...\")\n",
        "\n",
        "            # Combine title and content with better formatting\n",
        "            document = f\"Page Title: {data['title']}\\nSource URL: {data['url']}\\n\\nContent:\\n{data['content']}\"\n",
        "\n",
        "            # Clean text while preserving important formatting\n",
        "            cleaned_doc = re.sub(r'\\s+', ' ', document)\n",
        "\n",
        "            # Extract metadata for better source tracking\n",
        "            source_url = data['url']\n",
        "            title = data['title']\n",
        "\n",
        "            # STRATEGY 1: Create specialized micro-chunks for key facts (OPTIMIZED)\n",
        "            print(f\"  Creating micro-chunks...\")\n",
        "            self._create_micro_chunks_optimized(cleaned_doc, doc_idx, source_url, title)\n",
        "\n",
        "            # STRATEGY 2: Create regular overlapping chunks (existing logic)\n",
        "            print(f\"  Creating regular chunks...\")\n",
        "            chunk_count = 0\n",
        "            for i in range(0, len(cleaned_doc), self.CHUNK_SIZE - self.CHUNK_OVERLAP):\n",
        "                chunk_text = cleaned_doc[i:i + self.CHUNK_SIZE]\n",
        "\n",
        "                # Skip very short chunks\n",
        "                if len(chunk_text) < 150:\n",
        "                    continue\n",
        "\n",
        "                # Ensure chunks end at sentence boundaries when possible\n",
        "                if i + self.CHUNK_SIZE < len(cleaned_doc):\n",
        "                    # Try to end at a sentence\n",
        "                    last_period = chunk_text.rfind('.')\n",
        "                    last_question = chunk_text.rfind('?')\n",
        "                    last_exclamation = chunk_text.rfind('!')\n",
        "\n",
        "                    sentence_end = max(last_period, last_question, last_exclamation)\n",
        "                    if sentence_end > len(chunk_text) * 0.8:  # If sentence end is in last 20%\n",
        "                        chunk_text = chunk_text[:sentence_end + 1]\n",
        "\n",
        "                self.chunks.append({\n",
        "                    'text': chunk_text.strip(),\n",
        "                    'doc_id': doc_idx,\n",
        "                    'chunk_id': len(self.chunks),\n",
        "                    'source_url': source_url,\n",
        "                    'title': title,\n",
        "                    'chunk_type': 'regular'\n",
        "                })\n",
        "                chunk_count += 1\n",
        "\n",
        "            print(f\"  ✓ Created {chunk_count} regular chunks from this document\")\n",
        "\n",
        "        print(f\"✓ Created {len(self.chunks)} total enhanced chunks (including micro-chunks)\")\n",
        "\n",
        "    def _create_micro_chunks_optimized(self, document, doc_idx, source_url, title):\n",
        "        \"\"\"\n",
        "        OPTIMIZED: Fast micro-chunk creation that prevents regex backtracking issues.\n",
        "        \"\"\"\n",
        "\n",
        "        # Limit document size for regex processing to prevent hanging\n",
        "        if len(document) > 50000:  # If document is very long, process in sections\n",
        "            sections = [document[i:i+50000] for i in range(0, len(document), 45000)]\n",
        "        else:\n",
        "            sections = [document]\n",
        "\n",
        "        for section in sections:\n",
        "            # Simple, fast patterns (no complex quantifiers that cause backtracking)\n",
        "            quick_patterns = {\n",
        "                'tuition_cost': [\n",
        "                    r'\\$\\d{1,2},?\\d{3}\\s*per\\s*course',\n",
        "                    r'\\$\\d{2},?\\d{3}\\s*total',\n",
        "                    r'tuition[^.]{0,50}\\$\\d{1,2},?\\d{3}',\n",
        "                    r'\\$\\d{1,2},?\\d{3}[^.]{0,30}tuition'\n",
        "                ],\n",
        "                'scholarship_names': [\n",
        "                    r'Data Science Institute Scholarship',\n",
        "                    r'MS in Applied Data Science Alumni Scholarship',\n",
        "                    r'[A-Z][a-z]+\\s+[A-Z][a-z]+\\s+Scholarship'\n",
        "                ],\n",
        "                'deadlines': [\n",
        "                    r'deadline[^.]{0,100}(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}',\n",
        "                    r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}[^.]{0,50}deadline',\n",
        "                    r'application[^.]{0,50}due[^.]{0,50}\\d{1,2}/\\d{1,2}/\\d{4}'\n",
        "                ],\n",
        "                'urls': [\n",
        "                    r'https?://[^\\s<>\"{}|\\\\\\^`\\[\\]]{10,100}',\n",
        "                    r'www\\.[^\\s<>\"{}|\\\\\\^`\\[\\]]{5,50}'\n",
        "                ],\n",
        "                'contact_info': [\n",
        "                    r'contact[^.]{0,50}(?:Patrick|Jose)',\n",
        "                    r'(?:Patrick|Jose)[^.]{0,50}enrollment',\n",
        "                    r'advising[^.]{0,30}appointment'\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            # Process each pattern type quickly\n",
        "            for info_type, patterns in quick_patterns.items():\n",
        "                for pattern in patterns:\n",
        "                    try:\n",
        "                        # Add timeout protection and limit matches\n",
        "                        matches = list(re.finditer(pattern, section, re.IGNORECASE))[:5]  # Max 5 matches per pattern\n",
        "\n",
        "                        for match in matches:\n",
        "                            # Get context around the match (±200 chars)\n",
        "                            start = max(0, match.start() - 200)\n",
        "                            end = min(len(section), match.end() + 200)\n",
        "                            context = section[start:end].strip()\n",
        "\n",
        "                            # Clean up context boundaries (simple version)\n",
        "                            if len(context) > 100:  # Ensure meaningful context\n",
        "                                # Simple boundary cleanup\n",
        "                                if start > 0 and ' ' in context[:50]:\n",
        "                                    space_idx = context.find(' ')\n",
        "                                    context = context[space_idx:].strip()\n",
        "\n",
        "                                self.chunks.append({\n",
        "                                    'text': f\"KEY {info_type.upper()}: {context}\",\n",
        "                                    'doc_id': doc_idx,\n",
        "                                    'chunk_id': len(self.chunks),\n",
        "                                    'source_url': source_url,\n",
        "                                    'title': title,\n",
        "                                    'chunk_type': 'micro',\n",
        "                                    'info_type': info_type,\n",
        "                                    'priority': 1.0\n",
        "                                })\n",
        "\n",
        "                    except re.error:\n",
        "                        # Skip problematic patterns\n",
        "                        continue\n",
        "\n",
        "    def create_embeddings(self):\n",
        "        \"\"\"OPTIMIZED: Create vector embeddings with progress tracking.\"\"\"\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer(self.EMBEDDING_MODEL)\n",
        "\n",
        "        print(f\"Creating embeddings for {len(self.chunks)} chunks...\")\n",
        "        texts = [chunk['text'] for chunk in self.chunks]\n",
        "\n",
        "        # Create embeddings in smaller batches with progress tracking\n",
        "        batch_size = 16  # Reduced from 32 for better memory management\n",
        "        all_embeddings = []\n",
        "\n",
        "        total_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_num = (i // batch_size) + 1\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "            print(f\"Processing batch {batch_num}/{total_batches} ({len(batch_texts)} chunks)...\")\n",
        "\n",
        "            # Create embeddings for this batch\n",
        "            batch_embeddings = self.embedding_model.encode(\n",
        "                batch_texts,\n",
        "                show_progress_bar=False,  # Disable model's progress bar since we have our own\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "            all_embeddings.append(batch_embeddings)\n",
        "\n",
        "            print(f\"✓ Batch {batch_num} complete\")\n",
        "\n",
        "        print(\"Combining embeddings...\")\n",
        "        # Combine all embeddings\n",
        "        embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "        print(\"Creating FAISS index...\")\n",
        "        # Create FAISS index\n",
        "        dimension = embeddings.shape[1]\n",
        "        index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "        # Normalize for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        index.add(embeddings)\n",
        "\n",
        "        self.vector_store = {\n",
        "            'index': index,\n",
        "            'embeddings': embeddings,\n",
        "            'chunks': self.chunks\n",
        "        }\n",
        "\n",
        "        print(f\"✓ Vector index created with {dimension}-dimensional embeddings\")\n",
        "        print(f\"✓ Total chunks indexed: {len(self.chunks)}\")\n",
        "\n",
        "    def search_chunks(self, query: str, k: int = 8):\n",
        "        \"\"\"\n",
        "        Enhanced search with keyword boosting and priority weighting.\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            return []\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search with more results initially\n",
        "        scores, indices = self.vector_store['index'].search(query_embedding, min(k * 3, len(self.chunks)))\n",
        "\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(self.chunks):\n",
        "                result = self.chunks[idx].copy()\n",
        "                result['semantic_score'] = float(score)\n",
        "                results.append(result)\n",
        "\n",
        "        # Apply keyword boosting and priority weighting\n",
        "        enhanced_results = self._apply_keyword_boosting(query, results)\n",
        "\n",
        "        # Sort by enhanced score and return top k\n",
        "        enhanced_results.sort(key=lambda x: x['final_score'], reverse=True)\n",
        "\n",
        "        return enhanced_results[:k]\n",
        "\n",
        "    def _apply_keyword_boosting(self, query, results):\n",
        "        \"\"\"\n",
        "        Apply keyword boosting and priority weighting to search results.\n",
        "        \"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Define keyword boost mappings\n",
        "        keyword_boosts = {\n",
        "            'tuition': ['tuition', 'cost', 'fee', 'price', '$', 'dollar'],\n",
        "            'scholarship': ['scholarship', 'financial aid', 'funding', 'grant'],\n",
        "            'deadline': ['deadline', 'due date', 'application', 'submit', 'apply by'],\n",
        "            'contact': ['contact', 'appointment', 'advisor', 'advising', 'schedule', 'meet'],\n",
        "            'url': ['link', 'website', 'portal', 'apply', 'registration']\n",
        "        }\n",
        "\n",
        "        for result in results:\n",
        "            text_lower = result['text'].lower()\n",
        "\n",
        "            # Start with semantic score\n",
        "            final_score = result['semantic_score']\n",
        "\n",
        "            # Apply micro-chunk priority boost\n",
        "            if result.get('chunk_type') == 'micro':\n",
        "                final_score *= 1.5  # 50% boost for micro-chunks\n",
        "\n",
        "            # Apply keyword boosting\n",
        "            for category, keywords in keyword_boosts.items():\n",
        "                if any(keyword in query_lower for keyword in keywords):\n",
        "                    # Count keyword matches in the chunk\n",
        "                    matches = sum(1 for keyword in keywords if keyword in text_lower)\n",
        "                    if matches > 0:\n",
        "                        final_score *= (1 + 0.2 * matches)  # Boost based on keyword frequency\n",
        "\n",
        "            # Special boost for exact cost matches\n",
        "            if any(term in query_lower for term in ['tuition', 'cost', 'price', 'fee']):\n",
        "                if any(pattern in text_lower for pattern in ['$', 'dollar', 'per course', 'total']):\n",
        "                    final_score *= 1.3\n",
        "\n",
        "            # Special boost for scholarship name matches\n",
        "            if 'scholarship' in query_lower:\n",
        "                if any(name in text_lower for name in ['data science institute', 'alumni scholarship']):\n",
        "                    final_score *= 1.4\n",
        "\n",
        "            # Special boost for URL presence when contact/appointment mentioned\n",
        "            if any(term in query_lower for term in ['contact', 'appointment', 'schedule', 'advisor']):\n",
        "                if 'http' in text_lower or 'portal' in text_lower:\n",
        "                    final_score *= 1.3\n",
        "\n",
        "            result['final_score'] = final_score\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_enhanced_answer(self, query: str, chunks: List[Dict]):\n",
        "        \"\"\"\n",
        "        Enhanced answer generation with better context handling and fact extraction.\n",
        "        \"\"\"\n",
        "        if not self.openai_client:\n",
        "            return \"OpenAI client not initialized\"\n",
        "\n",
        "        # Separate micro-chunks and regular chunks\n",
        "        micro_chunks = [c for c in chunks if c.get('chunk_type') == 'micro']\n",
        "        regular_chunks = [c for c in chunks if c.get('chunk_type') != 'micro']\n",
        "\n",
        "        # Build context with micro-chunks prioritized\n",
        "        context_parts = []\n",
        "\n",
        "        # Add micro-chunks first (key facts)\n",
        "        if micro_chunks:\n",
        "            context_parts.append(\"KEY FACTS:\")\n",
        "            for i, chunk in enumerate(micro_chunks):\n",
        "                context_parts.append(f\"FACT {i+1}: {chunk['text']}\")\n",
        "            context_parts.append(\"\\nADDITIONAL CONTEXT:\")\n",
        "\n",
        "        # Add regular chunks\n",
        "        for i, chunk in enumerate(regular_chunks):\n",
        "            context_parts.append(f\"Source {i+1} (from {chunk['title']}):\\n{chunk['text']}\\n\")\n",
        "\n",
        "        context = \"\\n\".join(context_parts)\n",
        "\n",
        "        # Enhanced prompt with specific instructions for key facts\n",
        "        prompt = f\"\"\"You are an expert assistant for the MS in Applied Data Science program at the University of Chicago.\n",
        "\n",
        "Your task is to provide comprehensive, accurate answers based on the official program information provided below.\n",
        "\n",
        "CONTEXT FROM OFFICIAL UCHICAGO WEBSITE:\n",
        "{context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. **COSTS/TUITION**: If asking about costs, you MUST include exact dollar amounts (e.g., \"$5,967 per course\", \"$71,604 total tuition\")\n",
        "2. **SCHOLARSHIPS**: If asking about scholarships, you MUST mention specific scholarship names like \"Data Science Institute Scholarship\" and \"MS in Applied Data Science Alumni Scholarship\"\n",
        "3. **DEADLINES**: If asking about deadlines, provide ALL specific dates mentioned (format: Month Day, Year)\n",
        "4. **CONTACT/APPOINTMENTS**: If asking about scheduling or advising, include any URLs or portal links mentioned\n",
        "5. **EXACT QUOTES**: For key facts, use the exact wording from the source material\n",
        "6. **COMPLETENESS**: Provide all relevant details, not just summaries\n",
        "7. **STRUCTURE**: Use bullet points for lists (costs, deadlines, requirements)\n",
        "8. **SOURCE VERIFICATION**: If information seems incomplete, state \"Additional details may be available on the official website\"\n",
        "\n",
        "Based ONLY on the information provided above, give a complete and detailed answer:\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Use GPT-3.5-turbo with optimized parameters for factual accuracy\n",
        "            response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an expert assistant for the UChicago MS in Applied Data Science program. You MUST provide complete, factual answers with exact details (costs, dates, names, URLs) from the provided context. Never summarize or generalize key facts.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=1000,  # More tokens for complete answers\n",
        "                temperature=0.1,  # Very low temperature for maximum factual accuracy\n",
        "                top_p=0.9,\n",
        "                frequency_penalty=0.0,\n",
        "                presence_penalty=0.0\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            if \"quota\" in error_msg.lower() or \"429\" in error_msg:\n",
        "                return \"OpenAI API quota exceeded. Please add credits to your OpenAI account at https://platform.openai.com/account/billing\"\n",
        "            elif \"401\" in error_msg:\n",
        "                return \"Invalid OpenAI API key. Please check your API key.\"\n",
        "            else:\n",
        "                return f\"Error generating response: {error_msg}\"\n",
        "\n",
        "    def ask_question(self, query: str):\n",
        "        \"\"\"Enhanced question answering with better error handling.\"\"\"\n",
        "        if not self.is_initialized:\n",
        "            return \"System not initialized. Please run full setup first.\"\n",
        "\n",
        "        print(f\"Searching for: {query}\")\n",
        "\n",
        "        # Search for relevant chunks with more results\n",
        "        relevant_chunks = self.search_chunks(query, 5)\n",
        "\n",
        "        if not relevant_chunks:\n",
        "            return \"No relevant information found.\"\n",
        "\n",
        "        # Generate enhanced answer\n",
        "        answer = self.generate_enhanced_answer(query, relevant_chunks)\n",
        "\n",
        "        # Display results with better formatting\n",
        "        print(\"\\n\" + \"=\"*100)\n",
        "        print(\"ANSWER:\")\n",
        "        print(answer)\n",
        "        print(\"\\n\" + \"=\"*100)\n",
        "        print(\"SOURCES:\")\n",
        "        for i, chunk in enumerate(relevant_chunks):\n",
        "            print(f\"\\nSource {i+1} (Relevance: {chunk.get('final_score', chunk.get('semantic_score', 0)):.3f}):\")\n",
        "            print(f\"Title: {chunk['title']}\")\n",
        "            print(f\"URL: {chunk['source_url']}\")\n",
        "            print(f\"Content Preview: {chunk['text'][:300]}...\")\n",
        "            print(\"-\" * 80)\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def initialize_system(self, openai_api_key: str, max_pages: int = 15):\n",
        "        \"\"\"OPTIMIZED: Initialize the complete enhanced RAG system.\"\"\"\n",
        "        print(\"Initializing Enhanced RAG System for UChicago MS Program...\")\n",
        "\n",
        "        # Setup OpenAI\n",
        "        self.setup_openai(openai_api_key)\n",
        "\n",
        "        # Enhanced scraping\n",
        "        self.scrape_website(max_pages)\n",
        "\n",
        "        if not self.scraped_data:\n",
        "            print(\"Failed to scrape data\")\n",
        "            return False\n",
        "\n",
        "        # Create enhanced chunks (ONLY ONCE - no duplication)\n",
        "        self.create_enhanced_chunks()\n",
        "\n",
        "        # Create embeddings (optimized with progress tracking)\n",
        "        self.create_embeddings()\n",
        "\n",
        "        self.is_initialized = True\n",
        "        print(\"✓ Enhanced RAG System fully initialized and ready!\")\n",
        "        return True"
      ],
      "metadata": {
        "id": "4QcI-9MCUuNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENHANCED COLAB INTERFACE\n",
        "\n",
        "def create_enhanced_interface():\n",
        "    \"\"\"Create an enhanced interactive interface for Google Colab.\"\"\"\n",
        "\n",
        "    # Initialize enhanced system\n",
        "    rag_system = ImprovedRAGSystem()\n",
        "\n",
        "    print(\"Enhanced UChicago MS Data Science Q&A Bot\")\n",
        "\n",
        "    # Get OpenAI API key with better instructions\n",
        "    print(\"OpenAI API Key Required:\")\n",
        "    print(\"If you don't have credits, add $5-10 at: https://platform.openai.com/account/billing\")\n",
        "    print(\"This project will cost less than $2 total to run.\")\n",
        "    print()\n",
        "\n",
        "    api_key = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"API key required!\")\n",
        "        return None\n",
        "\n",
        "    # Initialize system with more pages for better coverage\n",
        "    success = rag_system.initialize_system(api_key, max_pages=15)\n",
        "\n",
        "    if not success:\n",
        "        print(\"Failed to initialize system\")\n",
        "        return None\n",
        "\n",
        "    print(\"Enhanced system ready! Now you'll get complete, accurate answers.\")\n",
        "    print(\"Type 'quit' to exit, 'examples' to see example questions\")\n",
        "\n",
        "    example_questions = [\n",
        "        \"What is the tuition cost for the program?\",\n",
        "        \"What scholarships are available for the program?\",\n",
        "        \"What are the core courses in the MS program?\",\n",
        "        \"What are the admission requirements?\",\n",
        "        \"Tell me about the capstone project\",\n",
        "        \"What are the application deadlines?\",\n",
        "        \"Who are the faculty members?\",\n",
        "        \"What career outcomes can I expect?\",\n",
        "        \"How long does the program take?\",\n",
        "        \"What are the prerequisites for admission?\"\n",
        "    ]\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        user_input = input(\"Ask a question: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        elif user_input.lower() == 'examples':\n",
        "            print(\"Example Questions:\")\n",
        "            for i, q in enumerate(example_questions, 1):\n",
        "                print(f\"{i}. {q}\")\n",
        "            continue\n",
        "        elif not user_input:\n",
        "            continue\n",
        "\n",
        "        rag_system.ask_question(user_input)\n",
        "\n",
        "    return rag_system"
      ],
      "metadata": {
        "id": "BnU4httsjBJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}